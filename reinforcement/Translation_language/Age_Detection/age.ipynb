{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Detetction with python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"person.jpg\"\n",
    "img = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect age\n",
    "predictions = DeepFace.analyze(img, actions=[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Age: [{'age': 30, 'region': {'x': 210, 'y': 39, 'w': 116, 'h': 116, 'left_eye': (287, 85), 'right_eye': (248, 85)}, 'face_confidence': 0.9}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted Age: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 36, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 40, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 31, 'region': {'x': 250, 'y': 119, 'w': 230, 'h': 230, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.99}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 36, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 35, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 36, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 34, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 34, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 34, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 34, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 34, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 36, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 34, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 35, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 31, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 32, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 34, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 26, 'region': {'x': 185, 'y': 54, 'w': 197, 'h': 197, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n",
      "Error: 'box'\n",
      "[{'age': 28, 'region': {'x': 167, 'y': 164, 'w': 249, 'h': 249, 'left_eye': (338, 252), 'right_eye': (240, 253)}, 'face_confidence': 0.96}]\n",
      "Error: 'box'\n",
      "[{'age': 29, 'region': {'x': 206, 'y': 115, 'w': 212, 'h': 212, 'left_eye': (346, 194), 'right_eye': (271, 194)}, 'face_confidence': 0.95}]\n",
      "Error: 'box'\n",
      "[{'age': 27, 'region': {'x': 204, 'y': 113, 'w': 219, 'h': 219, 'left_eye': (351, 196), 'right_eye': (274, 190)}, 'face_confidence': 0.96}]\n",
      "Error: 'box'\n",
      "[{'age': 28, 'region': {'x': 203, 'y': 99, 'w': 222, 'h': 222, 'left_eye': (350, 185), 'right_eye': (274, 181)}, 'face_confidence': 0.96}]\n",
      "Error: 'box'\n",
      "[{'age': 28, 'region': {'x': 190, 'y': 100, 'w': 220, 'h': 220, 'left_eye': (336, 181), 'right_eye': (258, 177)}, 'face_confidence': 0.96}]\n",
      "Error: 'box'\n",
      "[{'age': 30, 'region': {'x': -19, 'y': 11, 'w': 331, 'h': 331, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.96}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 37, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 33, 'region': {'x': 279, 'y': 85, 'w': 191, 'h': 191, 'left_eye': (360, 194), 'right_eye': (342, 156)}, 'face_confidence': 0.92}]\n",
      "Error: 'box'\n",
      "[{'age': 27, 'region': {'x': 228, 'y': 109, 'w': 199, 'h': 199, 'left_eye': (355, 183), 'right_eye': (296, 183)}, 'face_confidence': 0.94}]\n",
      "Error: 'box'\n",
      "[{'age': 28, 'region': {'x': 208, 'y': 167, 'w': 228, 'h': 228, 'left_eye': (357, 253), 'right_eye': (281, 249)}, 'face_confidence': 0.95}]\n",
      "Error: 'box'\n",
      "[{'age': 28, 'region': {'x': 203, 'y': 89, 'w': 268, 'h': 268, 'left_eye': (387, 185), 'right_eye': (284, 179)}, 'face_confidence': 0.96}]\n",
      "Error: 'box'\n",
      "[{'age': 31, 'region': {'x': 226, 'y': 11, 'w': 267, 'h': 267, 'left_eye': (412, 104), 'right_eye': (293, 92)}, 'face_confidence': 0.95}]\n",
      "Error: 'box'\n",
      "[{'age': 28, 'region': {'x': 215, 'y': 74, 'w': 234, 'h': 234, 'left_eye': (372, 163), 'right_eye': (292, 161)}, 'face_confidence': 0.96}]\n",
      "Error: 'box'\n",
      "[{'age': 27, 'region': {'x': 225, 'y': 77, 'w': 223, 'h': 223, 'left_eye': (371, 164), 'right_eye': (292, 159)}, 'face_confidence': 0.95}]\n",
      "Error: 'box'\n",
      "[{'age': 26, 'region': {'x': 225, 'y': 85, 'w': 221, 'h': 221, 'left_eye': (370, 170), 'right_eye': (291, 163)}, 'face_confidence': 0.96}]\n",
      "Error: 'box'\n",
      "[{'age': 28, 'region': {'x': 215, 'y': 88, 'w': 237, 'h': 237, 'left_eye': (371, 176), 'right_eye': (287, 172)}, 'face_confidence': 0.95}]\n",
      "Error: 'box'\n",
      "[{'age': 28, 'region': {'x': 209, 'y': 83, 'w': 245, 'h': 245, 'left_eye': (366, 175), 'right_eye': (283, 170)}, 'face_confidence': 0.94}]\n",
      "Error: 'box'\n",
      "[{'age': 27, 'region': {'x': 232, 'y': 78, 'w': 246, 'h': 246, 'left_eye': (386, 177), 'right_eye': (307, 168)}, 'face_confidence': 0.93}]\n",
      "Error: 'box'\n",
      "[{'age': 35, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 33, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 39, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n",
      "[{'age': 38, 'region': {'x': 0, 'y': 0, 'w': 640, 'h': 480, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0}]\n",
      "Error: 'box'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Perform face detection using DeepFace\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Print the result to check its structure\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\deepface\\DeepFace.py:247\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent, anti_spoofing)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze\u001b[39m(\n\u001b[0;32m    161\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    162\u001b[0m     actions: Union[\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m     anti_spoofing: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    169\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    Analyze facial attributes such as age, gender, emotion, and race in the provided image.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m            - 'white': Confidence score for White ethnicity.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdemography\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\deepface\\modules\\demography.py:172\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent, anti_spoofing)\u001b[0m\n\u001b[0;32m    169\u001b[0m     obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdominant_emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Emotion\u001b[38;5;241m.\u001b[39mlabels[np\u001b[38;5;241m.\u001b[39margmax(emotion_predictions)]\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 172\u001b[0m     apparent_age \u001b[38;5;241m=\u001b[39m \u001b[43mmodeling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# int cast is for exception - object of type 'float32' is not JSON serializable\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(apparent_age)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\deepface\\extendedmodels\\Age.py:36\u001b[0m, in \u001b[0;36mApparentAgeClient.predict\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m---> 36\u001b[0m     age_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find_apparent_age(age_predictions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\engine\\training.py:2650\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2648\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   2649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2650\u001b[0m     tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Open a connection to the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Perform face detection using DeepFace\n",
    "        result = DeepFace.analyze(frame, actions=[\"age\"], enforce_detection=False)\n",
    "\n",
    "        # Print the result to check its structure\n",
    "        print(result)\n",
    "\n",
    "        # Assuming result is a list of dictionaries, with each dictionary containing face data\n",
    "        for face in result:\n",
    "            # Extract face data\n",
    "            age = face[\"age\"]\n",
    "            x, y, w, h = face[\"box\"]\n",
    "\n",
    "            # Draw rectangles and text on detected faces\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"Age: {age}\",\n",
    "                (x, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.9,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Display the resulting frame\n",
    "    try:\n",
    "        cv2.imshow(\"Age Detection\", frame)\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying frame: {e}\")\n",
    "        break\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
